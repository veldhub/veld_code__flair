{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a6bddb-724e-47c4-9969-5eaed5a388e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-28 12:56:56,199 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n",
      "2025-01-28 12:56:58,772 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n",
      "2025-01-28 12:57:25,737 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n",
      "2025-01-28 12:57:35,906 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n",
      "2025-01-28 12:57:38,184 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n",
      "2025-01-28 12:57:40,100 SequenceTagger predicts: Dictionary with 4852 tags: <unk>, be.01, be.03, have.01, say.01, do.01, have.03, do.02, be.02, know.01, think.01, come.01, see.01, want.01, go.02, tell.01, give.01, use.01, make.02, take.01, talk.01, get.01, go.04, live.01, need.01, believe.01, work.01, mean.01, have.02, look.01, become.01, die.01, help.01, find.01, try.01, hear.01, know.06, show.01, happen.01, let.01, sell.01, bring.01, make.01, invest.01, begin.01, make.LV, continue.01, kill.01, speak.01, start.01\n",
      "2025-01-28 12:57:41,979 SequenceTagger predicts: Dictionary with 47 tags: O, S-NP, B-NP, E-NP, I-NP, S-VP, B-VP, E-VP, I-VP, S-PP, B-PP, E-PP, I-PP, S-ADVP, B-ADVP, E-ADVP, I-ADVP, S-SBAR, B-SBAR, E-SBAR, I-SBAR, S-ADJP, B-ADJP, E-ADJP, I-ADJP, S-PRT, B-PRT, E-PRT, I-PRT, S-CONJP, B-CONJP, E-CONJP, I-CONJP, S-INTJ, B-INTJ, E-INTJ, I-INTJ, S-LST, B-LST, E-LST, I-LST, S-UCP, B-UCP, E-UCP, I-UCP, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.nn import Classifier\n",
    "from flair.splitter import SegtokSentenceSplitter\n",
    "splitter = SegtokSentenceSplitter()\n",
    "tagger_ner = Classifier.load('ner')\n",
    "tagger_ner_english_fast = Classifier.load('flair/ner-english-fast')\n",
    "tagger_sent = Classifier.load('sentiment')\n",
    "tagger_ner_english_large = Classifier.load('flair/ner-english-large')\n",
    "tagger_linker = Classifier.load('linker')\n",
    "tagger_pos = Classifier.load('flair/pos-english')\n",
    "tagger_frame = Classifier.load('frame')\n",
    "tagger_chunk = Classifier.load('chunk')\n",
    "extractor_relations = Classifier.load('relations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bd52cda-ca53-424a-b1d8-8e49da8d0d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence[4]: \"I love Berlin .\"\n",
      "Sentence[4]: \"I love Berlin .\" → [\"Berlin\"/LOC]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'I love Berlin .',\n",
       " 'labels': [],\n",
       " 'entities': [{'text': 'Berlin',\n",
       "   'start_pos': 7,\n",
       "   'end_pos': 13,\n",
       "   'labels': [{'value': 'LOC', 'confidence': 0.9989738464355469}]}],\n",
       " 'relations': [],\n",
       " 'tokens': [{'text': 'I', 'start_pos': 0, 'end_pos': 1, 'labels': []},\n",
       "  {'text': 'love', 'start_pos': 2, 'end_pos': 6, 'labels': []},\n",
       "  {'text': 'Berlin', 'start_pos': 7, 'end_pos': 13, 'labels': []},\n",
       "  {'text': '.', 'start_pos': 14, 'end_pos': 15, 'labels': []}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = Sentence('I love Berlin .')\n",
    "print(sentence)\n",
    "tagger_ner.predict(sentence)\n",
    "print(sentence)\n",
    "sentence.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77ef1d92-276b-4a71-8507-fb4b7e4c43ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence[4]: \"I love Berlin .\"\n",
      "Sentence[4]: \"I love Berlin .\" → POSITIVE (0.9983)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'I love Berlin .',\n",
       " 'labels': [{'value': 'POSITIVE', 'confidence': 0.998337984085083}],\n",
       " 'entities': [],\n",
       " 'relations': [],\n",
       " 'tokens': [{'text': 'I', 'start_pos': 0, 'end_pos': 1, 'labels': []},\n",
       "  {'text': 'love', 'start_pos': 2, 'end_pos': 6, 'labels': []},\n",
       "  {'text': 'Berlin', 'start_pos': 7, 'end_pos': 13, 'labels': []},\n",
       "  {'text': '.', 'start_pos': 14, 'end_pos': 15, 'labels': []}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = Sentence('I love Berlin .')\n",
    "print(sentence)\n",
    "tagger_sent.predict(sentence)\n",
    "print(sentence)\n",
    "sentence.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5de5b5e6-44d0-4c49-acb4-3ea4f42e0555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[0]: \"The\"\n",
      "Token[1]: \"grass\"\n",
      "Token[2]: \"is\"\n",
      "Token[3]: \"green\"\n",
      "Token[4]: \".\"\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence('The grass is green.')\n",
    "for token in sentence:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ee3e718-8766-48da-af2d-c330cc26d23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence[5]: \"The grass is green.\" → [\"green\"/color]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'The grass is green.',\n",
       " 'labels': [],\n",
       " 'entities': [],\n",
       " 'relations': [],\n",
       " 'tokens': [{'text': 'The', 'start_pos': 0, 'end_pos': 3, 'labels': []},\n",
       "  {'text': 'grass', 'start_pos': 4, 'end_pos': 9, 'labels': []},\n",
       "  {'text': 'is', 'start_pos': 10, 'end_pos': 12, 'labels': []},\n",
       "  {'text': 'green',\n",
       "   'start_pos': 13,\n",
       "   'end_pos': 18,\n",
       "   'labels': [{'value': 'color', 'confidence': 1.0}]},\n",
       "  {'text': '.', 'start_pos': 18, 'end_pos': 19, 'labels': []}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = Sentence('The grass is green.')\n",
    "sentence[3].add_label('ner', 'color')\n",
    "print(sentence)\n",
    "sentence.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e24bae-3a7c-4469-8cc4-937cfc60294e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence[5]: \"The grass is green.\" → POSITIVE (1.0000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'The grass is green.',\n",
       " 'labels': [{'value': 'POSITIVE', 'confidence': 1.0}],\n",
       " 'entities': [],\n",
       " 'relations': [],\n",
       " 'tokens': [{'text': 'The', 'start_pos': 0, 'end_pos': 3, 'labels': []},\n",
       "  {'text': 'grass', 'start_pos': 4, 'end_pos': 9, 'labels': []},\n",
       "  {'text': 'is', 'start_pos': 10, 'end_pos': 12, 'labels': []},\n",
       "  {'text': 'green', 'start_pos': 13, 'end_pos': 18, 'labels': []},\n",
       "  {'text': '.', 'start_pos': 18, 'end_pos': 19, 'labels': []}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = Sentence('The grass is green.')\n",
    "sentence.add_label('sentiment', 'POSITIVE')\n",
    "print(sentence)\n",
    "sentence.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ebd3bf3-c70f-4893-85fc-b0b9f86dedb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence[5]: \"The grass is green.\" → POSITIVE (1.0)\n"
     ]
    }
   ],
   "source": [
    "for label in sentence.get_labels():\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22dca967-e617-493e-8a77-80ca7694eaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence[6]: \"George Washington went to Washington.\" → [\"George Washington\"/PER, \"Washington\"/LOC]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'George Washington went to Washington.',\n",
       " 'labels': [],\n",
       " 'entities': [{'text': 'George Washington',\n",
       "   'start_pos': 0,\n",
       "   'end_pos': 17,\n",
       "   'labels': [{'value': 'PER', 'confidence': 0.998886227607727}]},\n",
       "  {'text': 'Washington',\n",
       "   'start_pos': 26,\n",
       "   'end_pos': 36,\n",
       "   'labels': [{'value': 'LOC', 'confidence': 0.9942097663879395}]}],\n",
       " 'relations': [],\n",
       " 'tokens': [{'text': 'George', 'start_pos': 0, 'end_pos': 6, 'labels': []},\n",
       "  {'text': 'Washington', 'start_pos': 7, 'end_pos': 17, 'labels': []},\n",
       "  {'text': 'went', 'start_pos': 18, 'end_pos': 22, 'labels': []},\n",
       "  {'text': 'to', 'start_pos': 23, 'end_pos': 25, 'labels': []},\n",
       "  {'text': 'Washington', 'start_pos': 26, 'end_pos': 36, 'labels': []},\n",
       "  {'text': '.', 'start_pos': 36, 'end_pos': 37, 'labels': []}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = Sentence('George Washington went to Washington.')\n",
    "tagger_ner.predict(sentence)\n",
    "print(sentence)\n",
    "sentence.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f13a6451-5a40-4f91-8f87-788ca082c59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label.value is: \"PER\"\n",
      "label.score is: \"0.998886227607727\"\n",
      "the text of label.data_point is: \"George Washington\"\n",
      "\n",
      "label.value is: \"LOC\"\n",
      "label.score is: \"0.9942097663879395\"\n",
      "the text of label.data_point is: \"Washington\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for label in sentence.get_labels():\n",
    "    print(f'label.value is: \"{label.value}\"')\n",
    "    print(f'label.score is: \"{label.score}\"')\n",
    "    print(f'the text of label.data_point is: \"{label.data_point.text}\"\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9edbee62-5084-458e-968c-d66b6f7dd6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence[6]: \"George Washington went to Washington.\" → [\"George Washington\"/PER, \"Washington\"/LOC]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'George Washington went to Washington.',\n",
       " 'labels': [],\n",
       " 'entities': [{'text': 'George Washington',\n",
       "   'start_pos': 0,\n",
       "   'end_pos': 17,\n",
       "   'labels': [{'value': 'PER', 'confidence': 0.9999939799308777}]},\n",
       "  {'text': 'Washington',\n",
       "   'start_pos': 26,\n",
       "   'end_pos': 36,\n",
       "   'labels': [{'value': 'LOC', 'confidence': 0.9999961853027344}]}],\n",
       " 'relations': [],\n",
       " 'tokens': [{'text': 'George', 'start_pos': 0, 'end_pos': 6, 'labels': []},\n",
       "  {'text': 'Washington', 'start_pos': 7, 'end_pos': 17, 'labels': []},\n",
       "  {'text': 'went', 'start_pos': 18, 'end_pos': 22, 'labels': []},\n",
       "  {'text': 'to', 'start_pos': 23, 'end_pos': 25, 'labels': []},\n",
       "  {'text': 'Washington', 'start_pos': 26, 'end_pos': 36, 'labels': []},\n",
       "  {'text': '.', 'start_pos': 36, 'end_pos': 37, 'labels': []}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = Sentence('George Washington went to Washington.')\n",
    "tagger_ner_english_large.predict(sentence)\n",
    "print(sentence)\n",
    "sentence.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "445e5b37-b038-45b3-90a6-1a518179632b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span[0:1]: \"Kirk\" → James_T._Kirk (0.9969)\n",
      "Span[2:3]: \"Spock\" → Spock (0.9971)\n",
      "Span[6:7]: \"Enterprise\" → USS_Enterprise_(NCC-1701-D) (0.975)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'Kirk and Spock met on the Enterprise.',\n",
       " 'labels': [],\n",
       " 'entities': [{'text': 'Kirk',\n",
       "   'start_pos': 0,\n",
       "   'end_pos': 4,\n",
       "   'labels': [{'value': 'James_T._Kirk', 'confidence': 0.9968516230583191}]},\n",
       "  {'text': 'Spock',\n",
       "   'start_pos': 9,\n",
       "   'end_pos': 14,\n",
       "   'labels': [{'value': 'Spock', 'confidence': 0.997094988822937}]},\n",
       "  {'text': 'Enterprise',\n",
       "   'start_pos': 26,\n",
       "   'end_pos': 36,\n",
       "   'labels': [{'value': 'USS_Enterprise_(NCC-1701-D)',\n",
       "     'confidence': 0.9749534726142883}]}],\n",
       " 'relations': [],\n",
       " 'tokens': [{'text': 'Kirk', 'start_pos': 0, 'end_pos': 4, 'labels': []},\n",
       "  {'text': 'and', 'start_pos': 5, 'end_pos': 8, 'labels': []},\n",
       "  {'text': 'Spock', 'start_pos': 9, 'end_pos': 14, 'labels': []},\n",
       "  {'text': 'met', 'start_pos': 15, 'end_pos': 18, 'labels': []},\n",
       "  {'text': 'on', 'start_pos': 19, 'end_pos': 21, 'labels': []},\n",
       "  {'text': 'the', 'start_pos': 22, 'end_pos': 25, 'labels': []},\n",
       "  {'text': 'Enterprise', 'start_pos': 26, 'end_pos': 36, 'labels': []},\n",
       "  {'text': '.', 'start_pos': 36, 'end_pos': 37, 'labels': []}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = Sentence('Kirk and Spock met on the Enterprise.')\n",
    "tagger_linker.predict(sentence)\n",
    "for label in sentence.get_labels():\n",
    "    print(label)\n",
    "sentence.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a387c3e-950a-4f29-a8de-a24d4704e9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence[5]: \"Bayern played against Barcelona.\" → [\"Bayern\"/FC_Bayern_Munich, \"Barcelona\"/FC_Barcelona]\n",
      "{'text': 'Bayern played against Barcelona.', 'labels': [], 'entities': [{'text': 'Bayern', 'start_pos': 0, 'end_pos': 6, 'labels': [{'value': 'FC_Bayern_Munich', 'confidence': 0.9988178610801697}]}, {'text': 'Barcelona', 'start_pos': 22, 'end_pos': 31, 'labels': [{'value': 'FC_Barcelona', 'confidence': 0.9998371601104736}]}], 'relations': [], 'tokens': [{'text': 'Bayern', 'start_pos': 0, 'end_pos': 6, 'labels': []}, {'text': 'played', 'start_pos': 7, 'end_pos': 13, 'labels': []}, {'text': 'against', 'start_pos': 14, 'end_pos': 21, 'labels': []}, {'text': 'Barcelona', 'start_pos': 22, 'end_pos': 31, 'labels': []}, {'text': '.', 'start_pos': 31, 'end_pos': 32, 'labels': []}]}\n",
      "Sentence[7]: \"The match took place in Barcelona.\" → [\"Barcelona\"/Barcelona]\n",
      "{'text': 'The match took place in Barcelona.', 'labels': [], 'entities': [{'text': 'Barcelona', 'start_pos': 24, 'end_pos': 33, 'labels': [{'value': 'Barcelona', 'confidence': 0.9999071359634399}]}], 'relations': [], 'tokens': [{'text': 'The', 'start_pos': 0, 'end_pos': 3, 'labels': []}, {'text': 'match', 'start_pos': 4, 'end_pos': 9, 'labels': []}, {'text': 'took', 'start_pos': 10, 'end_pos': 14, 'labels': []}, {'text': 'place', 'start_pos': 15, 'end_pos': 20, 'labels': []}, {'text': 'in', 'start_pos': 21, 'end_pos': 23, 'labels': []}, {'text': 'Barcelona', 'start_pos': 24, 'end_pos': 33, 'labels': []}, {'text': '.', 'start_pos': 33, 'end_pos': 34, 'labels': []}]}\n"
     ]
    }
   ],
   "source": [
    "text = \"Bayern played against Barcelona. The match took place in Barcelona.\"\n",
    "sentences = splitter.split(text)\n",
    "tagger_linker.predict(sentences)\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print(sentence.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3143aab-4bb0-449c-8caa-53f27f29b9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence[6]: \"Dirk went to the store.\" → [\"Dirk\"/NNP, \"went\"/VBD, \"to\"/IN, \"the\"/DT, \"store\"/NN, \".\"/.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'Dirk went to the store.',\n",
       " 'labels': [],\n",
       " 'entities': [],\n",
       " 'relations': [],\n",
       " 'tokens': [{'text': 'Dirk',\n",
       "   'start_pos': 0,\n",
       "   'end_pos': 4,\n",
       "   'labels': [{'value': 'NNP', 'confidence': 0.9999831914901733}]},\n",
       "  {'text': 'went',\n",
       "   'start_pos': 5,\n",
       "   'end_pos': 9,\n",
       "   'labels': [{'value': 'VBD', 'confidence': 1.0}]},\n",
       "  {'text': 'to',\n",
       "   'start_pos': 10,\n",
       "   'end_pos': 12,\n",
       "   'labels': [{'value': 'IN', 'confidence': 0.9999363422393799}]},\n",
       "  {'text': 'the',\n",
       "   'start_pos': 13,\n",
       "   'end_pos': 16,\n",
       "   'labels': [{'value': 'DT', 'confidence': 1.0}]},\n",
       "  {'text': 'store',\n",
       "   'start_pos': 17,\n",
       "   'end_pos': 22,\n",
       "   'labels': [{'value': 'NN', 'confidence': 1.0}]},\n",
       "  {'text': '.',\n",
       "   'start_pos': 22,\n",
       "   'end_pos': 23,\n",
       "   'labels': [{'value': '.', 'confidence': 0.9999918937683105}]}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = Sentence('Dirk went to the store.')\n",
    "tagger_pos.predict(sentence)\n",
    "print(sentence)\n",
    "sentence.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bcc1525-ab20-4784-94e0-ec43f3861f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token[0]: \"George\"\n",
      "Token[1]: \"returned\" → return.01 (1.0000)\n",
      "Token[2]: \"to\"\n",
      "Token[3]: \"Berlin\"\n",
      "Token[4]: \"to\"\n",
      "Token[5]: \"return\" → return.02 (0.9759)\n",
      "Token[6]: \"his\"\n",
      "Token[7]: \"hat\"\n",
      "Token[8]: \".\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'George returned to Berlin to return his hat.',\n",
       " 'labels': [],\n",
       " 'entities': [],\n",
       " 'relations': [],\n",
       " 'tokens': [{'text': 'George', 'start_pos': 0, 'end_pos': 6, 'labels': []},\n",
       "  {'text': 'returned',\n",
       "   'start_pos': 7,\n",
       "   'end_pos': 15,\n",
       "   'labels': [{'value': 'return.01', 'confidence': 0.9999761581420898}]},\n",
       "  {'text': 'to', 'start_pos': 16, 'end_pos': 18, 'labels': []},\n",
       "  {'text': 'Berlin', 'start_pos': 19, 'end_pos': 25, 'labels': []},\n",
       "  {'text': 'to', 'start_pos': 26, 'end_pos': 28, 'labels': []},\n",
       "  {'text': 'return',\n",
       "   'start_pos': 29,\n",
       "   'end_pos': 35,\n",
       "   'labels': [{'value': 'return.02', 'confidence': 0.9759197235107422}]},\n",
       "  {'text': 'his', 'start_pos': 36, 'end_pos': 39, 'labels': []},\n",
       "  {'text': 'hat', 'start_pos': 40, 'end_pos': 43, 'labels': []},\n",
       "  {'text': '.', 'start_pos': 43, 'end_pos': 44, 'labels': []}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = Sentence('George returned to Berlin to return his hat.')\n",
    "tagger_frame.predict(sentence)\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "sentence.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1a4c872-9014-4d60-bed9-82e875240c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span[0:4]: \"The quick brown fox\" → NP (0.9978)\n",
      "Span[4:5]: \"jumps\" → VP (1.0)\n",
      "Span[5:6]: \"over\" → PP (0.9999)\n",
      "Span[6:9]: \"the lazy dog\" → NP (0.9986)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'The quick brown fox jumps over the lazy dog.',\n",
       " 'labels': [],\n",
       " 'entities': [{'text': 'The quick brown fox',\n",
       "   'start_pos': 0,\n",
       "   'end_pos': 19,\n",
       "   'labels': [{'value': 'NP', 'confidence': 0.9978180974721909}]},\n",
       "  {'text': 'jumps',\n",
       "   'start_pos': 20,\n",
       "   'end_pos': 25,\n",
       "   'labels': [{'value': 'VP', 'confidence': 0.9999922513961792}]},\n",
       "  {'text': 'over',\n",
       "   'start_pos': 26,\n",
       "   'end_pos': 30,\n",
       "   'labels': [{'value': 'PP', 'confidence': 0.9999420642852783}]},\n",
       "  {'text': 'the lazy dog',\n",
       "   'start_pos': 31,\n",
       "   'end_pos': 43,\n",
       "   'labels': [{'value': 'NP', 'confidence': 0.9986434777577718}]}],\n",
       " 'relations': [],\n",
       " 'tokens': [{'text': 'The', 'start_pos': 0, 'end_pos': 3, 'labels': []},\n",
       "  {'text': 'quick', 'start_pos': 4, 'end_pos': 9, 'labels': []},\n",
       "  {'text': 'brown', 'start_pos': 10, 'end_pos': 15, 'labels': []},\n",
       "  {'text': 'fox', 'start_pos': 16, 'end_pos': 19, 'labels': []},\n",
       "  {'text': 'jumps', 'start_pos': 20, 'end_pos': 25, 'labels': []},\n",
       "  {'text': 'over', 'start_pos': 26, 'end_pos': 30, 'labels': []},\n",
       "  {'text': 'the', 'start_pos': 31, 'end_pos': 34, 'labels': []},\n",
       "  {'text': 'lazy', 'start_pos': 35, 'end_pos': 39, 'labels': []},\n",
       "  {'text': 'dog', 'start_pos': 40, 'end_pos': 43, 'labels': []},\n",
       "  {'text': '.', 'start_pos': 43, 'end_pos': 44, 'labels': []}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = Sentence('The quick brown fox jumps over the lazy dog.')\n",
    "tagger_chunk.predict(sentence)\n",
    "for chunk in sentence.get_labels():\n",
    "  print(chunk)\n",
    "sentence.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a3d16c7-0bb8-4de4-acef-537f80c42f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span[0:1]: \"George\" → PER (0.9971)\n",
      "Span[4:5]: \"Washington\" → LOC (0.9847)\n",
      "Relation[0:1][4:5]: \"George -> Washington\" → born_in (1.0)\n",
      "Relation[0:1][4:5]: \"George -> Washington\" → born_in (1.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'George was born in Washington',\n",
       " 'labels': [],\n",
       " 'entities': [{'text': 'George',\n",
       "   'start_pos': 0,\n",
       "   'end_pos': 6,\n",
       "   'labels': [{'value': 'PER', 'confidence': 0.99710613489151}]},\n",
       "  {'text': 'Washington',\n",
       "   'start_pos': 19,\n",
       "   'end_pos': 29,\n",
       "   'labels': [{'value': 'LOC', 'confidence': 0.9846953749656677}]}],\n",
       " 'relations': [{'from_text': 'George',\n",
       "   'to_text': 'Washington',\n",
       "   'from_idx': 0,\n",
       "   'to_idx': 4,\n",
       "   'labels': [{'value': 'born_in', 'confidence': 0.9999638795852661}]}],\n",
       " 'tokens': [{'text': 'George', 'start_pos': 0, 'end_pos': 6, 'labels': []},\n",
       "  {'text': 'was', 'start_pos': 7, 'end_pos': 10, 'labels': []},\n",
       "  {'text': 'born', 'start_pos': 11, 'end_pos': 15, 'labels': []},\n",
       "  {'text': 'in', 'start_pos': 16, 'end_pos': 18, 'labels': []},\n",
       "  {'text': 'Washington', 'start_pos': 19, 'end_pos': 29, 'labels': []}]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = Sentence(\"George was born in Washington\")\n",
    "tagger_ner_english_fast.predict(sentence)\n",
    "entities = sentence.get_labels('ner')\n",
    "for entity in entities:\n",
    "    print(entity)\n",
    "extractor_relations.predict(sentence)\n",
    "relations = sentence.get_labels('relation')\n",
    "for relation in relations:\n",
    "    print(relation)\n",
    "for label in sentence.get_labels('relation'):\n",
    "    print(label)\n",
    "sentence.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c8f6d28-9aa2-473a-ad69-3a3730ad3408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence[5]: \"This is a sentence.\"\n",
      "{'text': 'This is a sentence.', 'labels': [], 'entities': [], 'relations': [], 'tokens': [{'text': 'This', 'start_pos': 0, 'end_pos': 4, 'labels': []}, {'text': 'is', 'start_pos': 5, 'end_pos': 7, 'labels': []}, {'text': 'a', 'start_pos': 8, 'end_pos': 9, 'labels': []}, {'text': 'sentence', 'start_pos': 10, 'end_pos': 18, 'labels': []}, {'text': '.', 'start_pos': 18, 'end_pos': 19, 'labels': []}]}\n",
      "Sentence[5]: \"This is another sentence.\"\n",
      "{'text': 'This is another sentence.', 'labels': [], 'entities': [], 'relations': [], 'tokens': [{'text': 'This', 'start_pos': 0, 'end_pos': 4, 'labels': []}, {'text': 'is', 'start_pos': 5, 'end_pos': 7, 'labels': []}, {'text': 'another', 'start_pos': 8, 'end_pos': 15, 'labels': []}, {'text': 'sentence', 'start_pos': 16, 'end_pos': 24, 'labels': []}, {'text': '.', 'start_pos': 24, 'end_pos': 25, 'labels': []}]}\n",
      "Sentence[4]: \"I love Berlin.\" → [\"Berlin\"/LOC]\n",
      "{'text': 'I love Berlin.', 'labels': [], 'entities': [{'text': 'Berlin', 'start_pos': 7, 'end_pos': 13, 'labels': [{'value': 'LOC', 'confidence': 0.9989738464355469}]}], 'relations': [], 'tokens': [{'text': 'I', 'start_pos': 0, 'end_pos': 1, 'labels': []}, {'text': 'love', 'start_pos': 2, 'end_pos': 6, 'labels': []}, {'text': 'Berlin', 'start_pos': 7, 'end_pos': 13, 'labels': []}, {'text': '.', 'start_pos': 13, 'end_pos': 14, 'labels': []}]}\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a sentence. This is another sentence. I love Berlin.\"\n",
    "sentences = splitter.split(text)\n",
    "tagger_ner.predict(sentences)\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print(sentence.to_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
